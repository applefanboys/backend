import requests
from bs4 import BeautifulSoup
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from App.core.database import get_db

from App.schemas.feed import FeedResponse, NewsItem, TopicCard, StockTip, ContentRequest
from App.ai_news.aiNews_service import build_user_keywords
from App.ai_news.recommendService import get_personalized_articles
# [NEW] AI ìš”ì•½(ëŒ€ë³¸ ìž‘ì„±) ê¸°ëŠ¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from App.tts.service import generate_shortform_script 

router = APIRouter(prefix="/api/feed", tags=["feed"])

# --- í¬ë¡¤ë§ í•¨ìˆ˜ ---
def crawl_news_content(url: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=3)
        if response.status_code != 200: return ""
        
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        
        content = []
        for p in paragraphs:
            text = p.get_text().strip()
            # ë³¸ë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸´ ë¬¸ìž¥ë§Œ ìˆ˜ì§‘
            if len(text) > 50: 
                content.append(text)
        
        full_text = " ".join(content)
        if len(full_text) < 100: return "" # ë„ˆë¬´ ì§§ìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
        return full_text
    except Exception:
        return ""

# [í•µì‹¬] ì‹¤ì‹œê°„ AI ìš”ì•½ API
@router.post("/content")
def get_news_content(req: ContentRequest):
    # 1. ì›ë¬¸ í¬ë¡¤ë§
    raw_text = crawl_news_content(req.url)
    
    if not raw_text:
        return {"content": "ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸ ë§í¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}
    
    # 2. [ì¤‘ìš”] ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì£¼ì§€ ì•Šê³ , AI(GPT)ì—ê²Œ ìš”ì•½ì„ ì‹œí‚µë‹ˆë‹¤.
    try:
        # ìµœëŒ€ 300ìž ì •ë„ì˜ ìˆí¼ ëŒ€ë³¸ìœ¼ë¡œ ìš”ì•½
        ai_summary = generate_shortform_script(raw_text, max_chars=300)
        return {"content": ai_summary}
    except Exception as e:
        print(f"AI Summary Error: {e}")
        # AI ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ì•žë¶€ë¶„ì´ë¼ë„ ë°˜í™˜
        return {"content": raw_text[:300] + "..."}


@router.get("/home", response_model=FeedResponse)
def get_home_feed(user_id: int, db: Session = Depends(get_db)):
    keywords = build_user_keywords(db, user_id)
    if not keywords: keywords = ["ê²½ì œ", "ì‚¼ì„±ì „ìž"]

    # [ìˆ˜ì • 1] per_keywordë¥¼ 30ìœ¼ë¡œ ëŠ˜ë ¤ì„œ 100ê°œ(limit)ë¥¼ ì¶©ë¶„ížˆ ì±„ì›€
    ai_articles = get_personalized_articles(
        user_keywords=keywords, 
        days=3, 
        limit=100, 
        per_keyword=30 
    )

    news_items = []
    for idx, art in enumerate(ai_articles):
        date_str = str(art.published_at)[:10] if art.published_at else ""
        
        # [ìˆ˜ì • 2] ì—¬ê¸°ì„œ í¬ë¡¤ë§í•˜ì§€ ì•ŠìŒ! (ì†ë„ í–¥ìƒ)
        # ë„¤ì´ë²„ ìš”ì•½ë¬¸(snippet)ì„ ìž„ì‹œë¡œ ë„£ì–´ë‘ê³ , í´ë¦­ ì‹œ ì‹¤ì‹œê°„ìœ¼ë¡œ AI ìš”ì•½ë³¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        temp_summary = art.description or "ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ë ¤ë©´ í´ë¦­í•˜ì„¸ìš”."

        news_items.append(NewsItem(
            id=idx,
            title=art.title,
            summary=temp_summary, 
            thumbnail_url="", 
            news_url=str(art.url),
            date=date_str,
            press=art.source or "ë„¤ì´ë²„ë‰´ìŠ¤"
        ))

    topics = [TopicCard(id=1, title="ðŸ’° ê¸ˆë¦¬ ì¸í•˜"), TopicCard(id=2, title="ðŸš€ ë°˜ë„ì²´")]
    stock_tips = [StockTip(code="005930", name="ì‚¼ì„±ì „ìž", description="AI ë°˜ë„ì²´")]

    return FeedResponse(
        topics=topics, news=news_items, trending=[], stockTips=stock_tips,
        fortune="ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ íˆ¬ìž ì§ê°ì´ ë§¤ìš° ë›°ì–´ë‚©ë‹ˆë‹¤!"
    )